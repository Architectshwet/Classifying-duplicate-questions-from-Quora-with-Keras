---
title: "R Notebook"
output: html_notebook
---
```{r}
setwd("C:/Users/Architect_shwet/Desktop/New folder/quora-question")
```



```{r}
library(keras)
```
We are using keras get_file() function to download the file
```{r}
quora_data <- get_file("quora_duplicate_questions.tsv",
  "https://qim.ec.quoracdn.net/quora_duplicate_questions.tsv")
```
We will first load data into R and do some preprocessing to make it easier to include in the model. After loading it we can read by readr function read_tsv
```{r}
library(readr)
df <- read_tsv(quora_data)
```

```{r}
head(df)
```
we will create a keras tokenizer to transform each word into an integer token. We will also specify a hyperparameter of our model, the vocabulary size. For now let's use 50,000 most common words. The tokenizer will be fit using all unique questions from the dataset
```{r}
tokenizer_quora <- text_tokenizer(num_words = 50000)
tokenizer_quora %>%
  fit_text_tokenizer(unique(c(df$question1, df$question2)))
```

```{r}
tokenizer_quora
```
let us save this tokenizer
```{r}
save_text_tokenizer(tokenizer_quora, 'tokenizer_quora_question')
```
We will now use this tokenizer to transform each question to numbers
```{r}
question1 <- texts_to_sequences(tokenizer_quora, df$question1)
question2 <- texts_to_sequences(tokenizer_quora, df$question2)
```

```{r}
length(question1)
```

```{r}
dim(df)
```
map_int - Apply a function to each element of a vector
```{r}
question1[1]
```

```{r}
head(df)
```
Lets take a look at the number of words in each question. This will help us to decide the padding length, another hyperparameter of our model. Padding the sequences normalizes them to the same size so that we can feed them to the keras model
```{r}
library(purrr)
questions_length <- c(
  map_int(question1, length),
  map_int(question2, length)
)
```

```{r}
length(questions_length)
```

```{r}
questions_length[1]
```

```{r}
quantile(questions_length, c(0.8, 0.9, 0.95, 0.99))
```
We can see that 99% of questions have at most length 31 so we'll choose a padding length between 15 and 30. Let's start with 20 (we'll also tune this parameter later). The default padding value is 0, but we are already using this value for words that don't appear within the 50,000 most frequent, so we'll use 50,001 instead.
keras Pads sequences to the same length
```{r}
question1_padded <- pad_sequences(question1, maxlen = 20, value = 50000+1)
question2_padded <- pad_sequences(question2, maxlen = 20, value = 50000+1)
```
length(question1)*20 = questions_padded
```{r}
length(question1_padded)
```

```{r}
question1_padded[404290 ,]
```
We have now finished the preprocessing steps. We will now run a simple benchmark model before moving on to the Keras model
Simple benchmark
Let's create two predictors: percentage of words from question1 that appear in question2 and vice-versa.
Then we will use a logistic regression to predict if the questions are duplicate or not.
```{r}
perc_words_question1 <- map2_dbl(question1, question2, ~mean(.x %in% .y))
perc_words_question2 <- map2_dbl(question2, question1, ~mean(.x %in% .y))
```

```{r}
df_model_bench <- data.frame(
  perc_words_question1 = perc_words_question1,
  perc_words_question2 = perc_words_question2,
  is_duplicate = df$is_duplicate
) %>%
  na.omit()
```

```{r}
val_sample <- sample.int(nrow(df_model_bench), 0.1*nrow(df_model_bench))
```

```{r}
library(caret)
logistic_regression <- glm(
  is_duplicate ~ perc_words_question1 + perc_words_question2,
  family = "binomial",
  data = df_model_bench[-val_sample,]
)

```

```{r}
summary(logistic_regression)
```

```{r}
pred <- predict(logistic_regression, df_model_bench[val_sample,], type = "response")
```

```{r}
pred <- pred > mean(df_model_bench$is_duplicate[-val_sample])
```

```{r}
accuracy <- table(pred, df_model_bench$is_duplicate[val_sample]) %>%
  prop.table() %>% 
  diag() %>% 
  sum()
accuracy
```
We will use a Siamese network to predict whether the pairs are duplicated or not. The idea is to create a model that can embed the questions(sequence of words) into a vector. Then we can compare the vectors for each question using a similarity measure
Lets define the part of the model that will embed the questions in a vector
```{r}
input1 <- layer_input( shape = c(20), name = "input_question1")
input2 <- layer_input( shape = c(20), name = "input_question2")
```
Then let us define the part of the model that will embed the questions into a vector.
```{r}
word_embedder <- layer_embedding(
  input_dim = 50000 + 1 + 1, #vocab size + UNK token + padding_value
  output_dim = 128, #hyperparameter - embedding size
  input_length = 20, #padding size
  embeddings_regularizer = regularizer_l2(0.0001) #hyperparameter tuning
)

seq_embedder <- layer_lstm(
  units = 128, # hyperparameter -- sequence embedding size
  kernel_regularizer = regularizer_l2(0.0001) #hyperparameter tuning
)
```
Now we will define the relationship between the input vectors and the embedding layers. Note that we use the same layers and weights on both inputs. That is why this is called siamese network. It makes sense, because we don't want to have different outputs if question1 is switched to question2
```{r}
vector1 <- input1 %>%
  word_embedder() %>%
  seq_embedder()
vector2 <- input2 %>%
  word_embedder() %>%
  seq_embedder()
```
We then define the similarity measure we want to optimize. We want duplicated questions to have higher values of similarity. We'll use cosine similarity, cosine similarity is the normalized dot product of the vectors,
```{r}
cosine_similarity <- layer_dot(list(vector1, vector2), axes = 1)
```
Next, we define a final sigmoid layer to output the probability of both questions being duplicated.
```{r}
output <- cosine_similarity %>%
  layer_dense(units = 1, activation = "sigmoid")
```
Now let us define the keras model in terms of its inputs and outputs and compile it. In the compilation phase we define our loss function and optimizer. We will use Adam optimizer to minimize the logloss
```{r}
model <- keras_model(list(input1, input2), output)
model %>% compile(
    optimizer = "adam",
    metrics = list(acc = metric_binary_accuracy),
    loss = "binary_crossentropy"
  )
```

```{r}
summary(model)
```
Model fitting
```{r}
set.seed(1817328)
val_sample <- sample.int(nrow(question1_padded), size = 0.1*nrow(question1_padded))
train_question1_padded <- question1_padded[-val_sample,]
train_question2_padded <- question1_padded[-val_sample,]
train_is_duplicate <- df$is_duplicate[-val_sample]

val_question1_padded <- question1_padded[val_sample,]
val_question2_padded <- question1_padded[val_sample,]
val_is_duplicate <- df$is_duplicate[val_sample]


```

```{r}
model %>%
  fit(
    list(train_question1_padded, train_question2_padded),
    train_is_duplicate,
    batch_size = 64,
    epochs = 10,
    validation_data = list(
      list(val_question1_padded, val_question2_padded),
      val_is_duplicate
    )
  )
```

```{r}
save_model_hdf5(model, "model-question-pairs.hdf5")
```
Now that we have a reasonable model, let's tune the hyperparameters using the tfruns package.
```{r}
flags_tune <- flags(
  flag_integer("vocab_size", 50000),
  flag_integer("max_len_padding", 20),
  flag_integer("embedding_size", 256),
  flag_numeric("regularization", 0.0001),
  flag_integer("seq_embedding_size", 512)
)
```

```{r}
input1 <- layer_input(shape = c(flags_tune$max_len_padding))
input2 <- layer_input(shape = c(flags_tune$max_len_padding))

embedding <- layer_embedding(
  input_dim = flags_tune$vocab_size + 2,
  output_dim = flags_tune$embedding_size,
  input_length = flags_tune$max_len_padding,
  embeddings_regularizer = regularizer_l2(l = flags_tune$regularization)
)
```
Now we are going to add an early stopping callback in the training step in order to stop training if validation loss doesn't decrease for 5 epochs in a row.
We also added a learning_rate reducer to reduce the learning rate by the factor of 10 when the loss doesn't decrease for 3 epochs 
```{r}
model %>% fit(
  ...,
  callbacks = list(
    callback_early_stopping(patience = 5),
    callback_reduce_lr_on_plateau(patience = 3)
  )
)
```
It's time to hypertune the hyperparameters using tuning_run() from tf_runs package.
```{r}
library(tfruns)

run <- tuning_run(
  "question-pairs.R",
  flags = list(
    vocab_size = c(30000, 40000, 50000, 60000),
    max_len_padding = c(15, 20, 25),
    embedding_size = c(64, 128, 256),
    regularization = c(0.00001, 0.0001, 0.001),
    seq_embedding_size = c(128,256, 512)
  ),
  runs_dir = "tuning",
  sample = 0.2
)
```
The tuning run will return a data.frame with results for all runs. We found that the best run attained 84.9% accuracy using the combination of hyperparameters shown below, so we modify our training script to use these values as the defaults:
```{r}
FLAGS <- flags(
  flag_integer("vocab_size", 50000),
  flag_integer("max_len_padding", 20),
  flag_integer("embedding_size", 256),
  flag_numeric("regularization", 1e-4),
  flag_integer("seq_embedding_size", 512)
)
```
Now that we have trained and tuned our model we can start making predictions. At prediction time we will load bot the model and text tokenizer file tha we  have save todisk earlier
```{r}
model <- load_model_hdf5("model-question-pairs.hdf5", compile = FALSE)
tokenizer <- load_text_tokenizer("tokenizer_quora_question")
```
Since we won't continue training the model, we specified the compile = FALSE 

Now let us define the function to create predictions. In this function, we preprocess the input data in the same way we preprocessed the training data 
```{r}
predict_question_pairs <- function(model, tokenizer, q1, q2) {
  q1 <- texts_to_sequences(tokenizer, list(q1))
  q2 <- texts_to_sequences(tokenizer, list(q2))
  
  q1 <- pad_sequences(q1,20)
  q2 <- pad_sequences(q2,20)
  
  as.numeric(predict(model, list(q1,q2)))
}
```

```{r}
predict_question_pairs(
  model,
  tokenizer,
  "What's R programming?",
  "What's R in programming?"
)
```




